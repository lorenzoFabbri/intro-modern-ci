[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Modern Causal Inference",
    "section": "",
    "text": "Goals and Approach\nOur goal is to get people with any amount of experience in formal mathematics past undergraduate probability to be able to take a vaguely-defined scientific question, translate it into a formal statistical problem, and solve it by constructing a tool that is optimal for the job at hand.\nThis book is not particularly original! There are many texts and courses that share the same philosophy and have substantial overlap in terms of the pedagogic approach and topics. In particular, we owe a lot to the biostatistics curriculum at UC Berkeley and to more encyclopedic texts like Van der Laan et al. (2011). Nonetheless, we think that the particular selection of topics and tone in this book will make the content accessible enough to new audiences that it’s worthwhile to recombine the material in this way. Think of this book as just another open window into the exciting world of modern causal inference.\nThis book aims to be a relatively self-contained treatment of the modern approach to causal inference. As we said above, the goal is for you to be able to take a vaguely-defined scientific question, translate it into a formal statistical problem, and solve it using a tool that you optimally construct for the job at hand. Thankfully, there’s a well-developed process for doing that called the Statistical Roadmap (Petersen & Laan, 2014) which we follow closely in the chapters of this book:\nThe authors thank Lauren Liao and Kat Hoffman for productive comments on chapter drafts."
  },
  {
    "objectID": "index.html#philosophy",
    "href": "index.html#philosophy",
    "title": "Introduction to Modern Causal Inference",
    "section": "Philosophy",
    "text": "Philosophy\nThis book is rooted in the philosophy of modern causal inference. What sets this philosophy apart are the three following tenets:\n\nThe first is that for all practical purposes, the point of statistics is causal inference. Ultimately, we humans are concerned with how to make decisions under uncertainty that lead to the best outcome. These are fundamentally causal questions that ask what if we did A instead of B? The machinery of statistical inference is agnostic to causality, but that doesn’t change the fact that our motivation in using it isn’t. Therefore there is little point in shying away from causal claims because noncausal claims are not often of any practical utility.\nThat leads to the second, often misunderstood point, which is that there is no such thing as a method for causal inference. This isn’t a failing of the statistics literature so much as a failing of the naive popularization of new ideas (e.g. blog posts, low-quality papers). The process of statistical inference (for point estimation) only cares about estimating a parameter of a probability distribution and quantifying the sampling distribution of that estimate. There is nothing causal about that, and many are surprised to learn that the algorithms used for causal inference are actually identical to those used to make noncausal statements. What makes an analysis causal has little if anything to do with the estimator or algorithm used. It has everything to do with whether or not the researcher can satisfy certain non-testable assumptions about the process that generated the observed data. It’s important to keep these things (statistical inference and causal identification) separated in your mind even though they must always work together.\nThe last and perhaps greatest shortcoming of traditional statistics pedagogy is that it generally does not teach you to ask the question that makes sense scientifically and then translate it into a statistical formalism. This is partly because, in the past, our analytic tools were quite limited and no progress could be made unless one imposed unrealistic assumptions or changed the question to fit the existing methods. Today, however, we have powerful tools that can then translate a given statistical problem into an optimal method for estimation in a wide variety of settings. It’s not always totally automatic, but at a minimum it provides a clear way to think about what’s better and what’s worse. It liberates you from the route if-this-kind-of-data-then-this-method thinking that only makes for bad statistics and bad science.\n\nTaken together, these three points give this movement a clear, unified, and increasingly popular perspective on causal inference. We have no doubt whatsoever that this is the paradigm that will come to dominate common practice in the next decades and century."
  },
  {
    "objectID": "index.html#pedagogy",
    "href": "index.html#pedagogy",
    "title": "Introduction to Modern Causal Inference",
    "section": "Pedagogy",
    "text": "Pedagogy\nThe modern approach to causal inference is already well-established and discussed at length in many high-quality resources and courses. So why have yet another book going over the same material?\nWell, there are many different kinds of students with different backgrounds for whom different presentation styles work better! We’ve chosen an approach for this particular text that focuses on the following principles:\n\nRigor with fewer prerequisites. Many students are turned off or intimidated by the amount of mathematical formality that is required to rigorously understand modern causal inference. While you don’t actually need a lot of math to use and understand the ideas, you do need some math to be able to work independently in the field. Since one of our goals is for you to be able to construct your own efficient estimator for a never-before-seen problem, we have to think precisely and not hand-wave away rigor.\nFor better or worse, the math you need is built on several layers of prerequisites- real analysis, probability theory, functional analysis, and asymptotic statistics (more on that later). In an ideal world, we would all have taken these courses and be prepared for what’s next, but in practice we know many people have gaps in their knowledge.\nIn this book we try to address that by providing a lot of foundational background information about these topics as they come up. We do our best to build up intuition for the component pieces of things before diving into implications. We lean heavily on figures and natural English prose to explain these concepts alongside the formal mathematical notation. At a minimum, we try to be very clear about what the foundations actually are so that if you don’t understand something you at least know where to look.\nCore concepts. Modern causal inference is a huge field. There are so many exciting applications and theoretical developments that it can be hard for students to find their way at first.\nWe’ve tried to keep thing as short and concise as possible in this book by focusing on core ideas and a limited number of examples. The point of this is to emphasize what is most important and to allow you to learn the fundamentals without getting distracted. That means there are many topics that aren’t covered in this book (see section below), but the idea is that this book will give you the tools and mindset you need in order to continue your learning productively.\nA welcoming voice. Modern causal inference is fast-moving and intense. It’s easy to feel like you don’t belong or aren’t good enough to participate. The agnostic, impersonal tone that is best for writing a clear academic paper doesn’t always create the warm, emotional connection that is necessary for us to feel secure and ready to learn.\nTo deal with that problem, the voice we use throughout this book is informal and decidedly nonacademic. We write in the first person and address you, the reader, directly. Figures are hand-drawn and cartoonish and we have links rather than formal citations. We provide personal commentary alongside agnostic technical material. All of this is deliberate and meant to help you feel comfortable and guided through this world. You are not alone in learning!\n\nOther books may share some or all of these principles to some extent so we’re not claiming that what we’ve done here is entirely unique. We’re trying to create as many inlets as possible for students with different lived experiences. If the approach we take here works for you, that’s great!\nIf you find this book isn’t for you, that’s also great! Here is a list of other resources that cover some of the same ground:\n\nSemiparametric Doubly Robust Targeted Double Machine Learning: A Review (Kennedy, 2022)\nSemiparametric Theory and Missing Data (Tsiatis, 2006)\nTargeted Learning (Van der Laan et al., 2011)\nUnified Methods for Censored Longitudinal Data and Causality (Laan & Robins, 2003)\nCausal Inference: What If? (Hernán & Robins, 2010)\n\n\nRigor with Fewer Prerequisites\nDespite our best efforts, you will need some background material to get the most out of this book. The hard prerequisites listed are non-negotiable (this book will make no sense to you at all if you don’t know what a probability distribution is), but the soft prerequisites are completely optional. We don’t cover these topics here because there are already so many good resources to learn them and there’s no need to reinvent the wheel. We provide the curated references below so you know where to go to fill in the holes in your understanding as you go along.\nHard Prerequisites   You’ll need to know probability theory at the undergraduate level. Random variables, expectation, conditional expectation, probability densities should be familiar to you, and ideally you have at least a vague understanding of what convergence in distribution means.\nYou also need undergraduate multivariable calculus and linear algebra: integrals, derivatives, vectors, etc. should all be well-worn tools at your disposal.\nIf you’re shaky on these topics, we recommend Khan Academy. We also encourage you to check out the real analysis and measure-theoretic probability resources listed below, which will provide you with a much more useful, conceptual understanding.\nSoft Prerequisites   The following topics aren’t necessary to grasp the arc of the arguments we’ll make, but a fully rigorous understanding isn’t possible without them. As we go along, we’ll provide parenthetical commentary to elucidate concepts from these subjects that you may not be familiar with. We suggest attempting to read and understand this book all the way through and taking notes on the places where you feel like you’re missing something. This will help you prioritize your time if you choose to fill in the gaps. None of this material is beyond you, we promise. It just takes a little time to work through it.\nThese books and videos give what we think is the most self-contained treatment of their respective topics that is in line with the didactic approach taken in this book. They tend to have a more conversational tone, provide more background and intuition to the reader, and have plenty of worked examples. As always, however, there are many alternatives that may work better for you depending on your needs. These subjects should be tackled in the order they are listed here:\n\n\n\nField\nTopics to Focus On\nResources\n\n\n\n\nIntro to Real Analysis\nsequences, limits, series, continuity, convergence (pointwise, uniform), derivative\nUnderstanding Analysis (Ch 1-6) (Abbott, 2015)\n\n\nMeasure-Theoretic Probability\ndefinition and construction of measure, Lebesgue integral and its properties, Radon-Nikodym theorem, change-of-variables formula, modes of convergence for random variables, independence, central limit theorem, conditional expectation\nMeasure Theory YouTube Lectures - Landim  Measure, Integral, and Probability (Capiński & Kopp, 2004)\n\n\nFunctional Analysis in \\(\\mathscr{L}_2\\)\nprojection, bounded dual space, Reisz representation\nIntroductory Functional Analysis with Applications (Ch 1-3) (Kreyszig, 1991)\n\n\n\nA few tips for self study: we recommend reading skimming each chapter first and getting an idea of what the main ideas are and how they interconnect to each other and to what you’ve already done. Take your time to understand what the definitions of the objects are that are used in each theorem (e.g. what exactly is a sequence / random variable / linear bounded functional?). Think of some examples and draw pictures.\nThen go through the chapter again and read it slowly, line by line. Reading a line of math should take you as long as it takes to read a paragraph of English because that’s often how much information is conveyed therein. Now draw some pictures or think of examples attempting the illustrate the content of the theorems. After you’re done, do several of the exercises that the author provides. We’ve found that it also helps to make a one-page summary or cheatsheet of all the material in the chapter (definitions, important theorems, and their relationships). This will help you keep all the concepts organized as you go along.\nWe’ll also refer to Asymptotic Statistics (Van der Vaart, 2000) in certain places throughout the guide where we need to point to technical material. When we do this, we’ll cite it as vdV 98. Relative to the way we write things up here, vdV 98 goes over background material much faster, so you may find it more challenging to read through. However, it does contain all the necessary information and details. If you’ve read the material in the table above and you’re feeling up to it you can try your hand at ch. 1, 2, 6, 7, 8, and 25 of that book.\n\n\nCore Concepts\nWe’ve done our best to keep this book lean and targeted on foundational concepts. As a consequence, there are many important and interesting topics that we’ll say nothing or little about. For the sake of completeness, we’ll list some of them here so you know where to start looking when you’re ready to build on your solid foundation.\nTo keep things simple and consistent, most of the examples in this book focus on inference of the average treatment effect in simple covariate-treatment-outcome observational or randomized studies. However, there are many data structures which are much more elaborate than this. For example, we do not fully discuss inference in longitudinal settings with time-varying covariates and treatments, inference when some outcomes are censored, stochastic interventions, or continuous treatments. Non-IID data (e.g. treatment spillover in a network) is another interesting topic that is not covered. Nonetheless, what is in this book will prepare you to study all of these topics and to understand them as special cases or extensions of the general approach presented here.\nWhile the tools we describe in this book are extremely powerful, they are not all-powerful. Some estimands of legitimate interest are not pathwise differentiable and therefore cannot be analyzed with these tools. An important example of such an estimand is the conditional average treatment effect, which is the subject of extensive research. There is not yet a unified approach to these problems.\nAnother interesting and useful setting that we do not discuss is online (or reinforcement) learning. Many topics related to this literature (e.g., off-policy evaluation) are directly addressable with the methods in this book, but here we omit all discussion of scenarios where the treatment policy can be manipulated during data collection as more information comes in. The framework for causal inference we develop in this book does extend to such settings but only with some added complexity that is best understood after learning the basics.\nLastly, the reader should be aware that this book discusses the most popular and widely used framework for causal inference, which is that of potential outcomes in the frequentist population model. Readers may have heard of directed acyclic graphs (DAGs) as an alternative to potential outcomes, but these two frameworks are in fact equivalent and easily unified (Richardson & Robins, 2013). Alternatives are exceedingly rare. Potential outcomes are also used outside the population model in the randomization inference framework, which differs in that only the treatment assignment is assumed to be random (i.e., sampling from a larger population is a source of variability). Much of what you learn here is also applicable to randomization inference. Potential outcomes are also commonly used in Bayesian approaches to causal inference, which we don’t discuss here."
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, S. (2015). Understanding analysis. Springer.\n\n\nCapiński, M., & Kopp, P. E. (2004). Measure, integral and\nprobability (Vol. 14). Springer.\n\n\nHernán, M. A., & Robins, J. M. (2010). Causal inference.\nCRC Boca Raton, FL.\n\n\nKennedy, E. H. (2022). Semiparametric doubly robust targeted double\nmachine learning: A review. arXiv Preprint arXiv:2203.06469.\n\n\nKreyszig, E. (1991). Introductory functional analysis with\napplications (Vol. 17). John Wiley & Sons.\n\n\nLaan, M. J., & Robins, J. M. (2003). Unified methods for\ncensored longitudinal data and causality. Springer.\n\n\nPetersen, M. L., & Laan, M. J. van der. (2014). Causal models and\nlearning from data: Integrating causal modeling and statistical\nestimation. Epidemiology (Cambridge, Mass.), 25(3),\n418.\n\n\nRichardson, T. S., & Robins, J. M. (2013). Single world intervention\ngraphs (SWIGs): A unification of the counterfactual and graphical\napproaches to causality. Center for the Statistics and the Social\nSciences, University of Washington Series. Working Paper,\n128(30), 2013.\n\n\nTsiatis, A. A. (2006). Semiparametric theory and missing data.\n\n\nVan der Laan, M. J., Rose, S., et al. (2011). Targeted learning:\nCausal inference for observational and experimental data (Vol. 4).\nSpringer.\n\n\nVan der Vaart, A. W. (2000). Asymptotic statistics (Vol. 3).\nCambridge university press."
  }
]